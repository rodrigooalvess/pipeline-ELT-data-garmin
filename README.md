# ğŸƒâ€â™‚ï¸ Garmin Data Pipeline: De Dados Brutos a Insights de Performance

Este projeto Ã© uma soluÃ§Ã£o completa de **ELT (Extract, Load, Transform)** projetada para automatizar a ingestÃ£o e anÃ¡lise de dados de atividades fÃ­sicas exportadas do Garmin. O pipeline transforma arquivos CSV brutos em uma camada analÃ­tica pronta para BI, utilizando as boas prÃ¡ticas de Engenharia de Dados.



## ğŸ¯ Objetivo
Automatizar o processamento de mÃ©tricas de corrida (Pace, DistÃ¢ncia, FC), garantindo a qualidade dos dados atravÃ©s de testes automatizados e orquestraÃ§Ã£o profissional com Prefect.

## ğŸ› ï¸ Tecnologias Utilizadas
* **Linguagem:** Python 3.13+
* **Gerenciamento de DependÃªncias:** Poetry
* **Banco de Dados:** PostgreSQL
* **TransformaÃ§Ã£o de Dados:** dbt (Data Build Tool)
* **OrquestraÃ§Ã£o:** Prefect
* **Qualidade:** dbt-tests

---

## ğŸ—ï¸ Arquitetura de Dados
O projeto segue a arquitetura medalhÃ£o:
1.  **Landing:** Arquivos CSV brutos recebidos na pasta `data/landing`.
2.  **Raw (Bronze):** Dados ingeridos via Python sem transformaÃ§Ãµes no schema `raw`.
3.  **Analytics (Silver/Gold):** Dados limpos, tipados e agregados via dbt no schema `analytics`.

---

## ğŸš€ Como Executar o Projeto

### 1. PrÃ©-requisitos
* PostgreSQL instalado localmente.
* Poetry instalado (`pip install poetry`).

### 2. ConfiguraÃ§Ã£o do Banco de Dados
No seu PostgreSQL, crie um banco de dados e os schemas necessÃ¡rios:
```sql
CREATE DATABASE garmin_db;

-- Conecte ao banco e crie os schemas
CREATE SCHEMA raw;
CREATE SCHEMA analytics;
```

### 3. ExecuÃ§Ã£o do Pipeline
Para processar os dados, insira seus arquivos `.csv` na pasta `data/landing` e execute o comando:

> **Dica:** JÃ¡ deixei um arquivo de exemplo dentro da pasta `data/landing` para que vocÃª possa testar o pipeline imediatamente! ğŸš€

```bash
poetry run python main.py
```

---

## ğŸ—ºï¸ Roadmap de EvoluÃ§Ã£o

Este projeto estÃ¡ em desenvolvimento contÃ­nuo. As prÃ³ximas etapas planejadas para a evoluÃ§Ã£o da arquitetura sÃ£o:

### ğŸŸ¢ Fase 1: VisualizaÃ§Ã£o e BI (Em breve)
- [ ] Conectar a tabela `fct_treinos_diarios` ao **Power BI**.
- [ ] Criar dashboards de performance com KPIs de Pace MÃ©dio, Volume Semanal e Zonas de FrequÃªncia CardÃ­aca.

### ğŸŸ£ Fase 2: InteligÃªncia Artificial e Feedback AutomÃ¡tico 
- [ ] Implementar integraÃ§Ã£o com LLMs para analisar os treinos e gerar feedbacks personalizados.
- [ ] Desenvolver modelos de Machine Learning para prever o tempo estimado de conclusÃ£o para distÃ¢ncias especÃ­ficas com base no histÃ³rico de treinos.
- [ ] Usar ML para identificar treinos com mÃ©tricas fora do padrÃ£o que possam indicar fadiga ou risco de lesÃ£o.

### ğŸŸ¡ Fase 3: Infraestrutura e Nuvem
- [ ] Migrar o banco de dados local para uma instÃ¢ncia gerenciada na **Cloud (GCP ou AWS)**.
- [ ] Implementar o armazenamento de arquivos brutos em Buckets (S3/GCS) para simular um Data Lake real.
- [ ] Containerizar a aplicaÃ§Ã£o utilizando **Docker** para facilitar o deploy.

### ğŸŸ  Fase 4: OrquestraÃ§Ã£o AvanÃ§ada
- [ ] Avaliar a migraÃ§Ã£o do Prefect para o **Apache Airflow** para gerenciar fluxos de dados mais complexos.
- [ ] Implementar monitoramento de **Data Quality** mais rigoroso com a biblioteca **Great Expectations**.

### ğŸ”µ Fase 5: Interface do UsuÃ¡rio (SaaS)
- [ ] Desenvolver uma Web App para que outros usuÃ¡rios possam realizar o upload de seus CSVs e visualizar relatÃ³rios e feedbacks da IA instantaneamente.
